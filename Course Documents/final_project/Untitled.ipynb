{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bd5b7c6b61c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[1;31m### remove any outliers before proceeding further\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"salary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"bonus\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mdata_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TOTAL'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatureFormat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# %load poi_id.py\n",
    "#!/usr/bin/python\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "from time import time\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat\n",
    "from feature_format import targetFeatureSplit\n",
    "\n",
    "\n",
    "### features_list is a list of strings, each of which is a feature name\n",
    "### first feature must be \"poi\", as this will be singled out as the label\n",
    "\n",
    "features_list = [\"poi\", \"fraction_from_poi_email\", \"fraction_to_poi_email\", 'shared_receipt_with_poi']\n",
    "\n",
    "### load the dictionary containing the dataset\n",
    "data_dict=[]\n",
    "with open(\"final_project_dataset.pkl\", 'rb') as f:\n",
    "     while True:\n",
    "        try:\n",
    "            data_dict.append(pickle.load(f))\n",
    "        except EOFError:\n",
    "            break\n",
    "\n",
    "### look at data\n",
    "#print len(data_dict.keys())\n",
    "#print data_dict['BUY RICHARD B']\n",
    "#print data_dict.values()\n",
    "\n",
    "\n",
    "### remove any outliers before proceeding further\n",
    "features = [\"salary\", \"bonus\"]\n",
    "data_dict.pop('TOTAL', 0)\n",
    "data = featureFormat(data_dict, features)\n",
    "\n",
    "### remove NAN's from dataset\n",
    "outliers = []\n",
    "for key in data_dict:\n",
    "    val = data_dict[key]['salary']\n",
    "    if val == 'NaN':\n",
    "        continue\n",
    "    outliers.append((key, int(val)))\n",
    "\n",
    "outliers_final = (sorted(outliers,key=lambda x:x[1],reverse=True)[:4])\n",
    "### uncomment for printing top 4 salaries\n",
    "### print outliers_final\n",
    "\n",
    "\n",
    "### plot features\n",
    "for point in data:\n",
    "    salary = point[0]\n",
    "    bonus = point[1]\n",
    "    plt.scatter( salary, bonus )\n",
    "\n",
    "plt.xlabel(\"salary\")\n",
    "plt.ylabel(\"bonus\")\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### create new features\n",
    "### new features are: fraction_to_poi_email,fraction_from_poi_email\n",
    "\n",
    "def dict_to_list(key,normalizer):\n",
    "    new_list=[]\n",
    "\n",
    "    for i in data_dict:\n",
    "        if data_dict[i][key]==\"NaN\" or data_dict[i][normalizer]==\"NaN\":\n",
    "            new_list.append(0.)\n",
    "        elif data_dict[i][key]>=0:\n",
    "            new_list.append(float(data_dict[i][key])/float(data_dict[i][normalizer]))\n",
    "    return new_list\n",
    "\n",
    "# After cleaning the data from outliers I had to pick the most sensible features to use.\n",
    "# First I picked 'from_poi_to_this_person' and 'from_this_person_to_poi' but there is was\n",
    "# no strong pattern when I plotted the data so I used fractions for both\n",
    "# features of \"from/to poi messages\" and \"total from/to messages\".\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### create two lists of new features\n",
    "fraction_from_poi_email=dict_to_list(\"from_poi_to_this_person\",\"to_messages\")\n",
    "fraction_to_poi_email=dict_to_list(\"from_this_person_to_poi\",\"from_messages\")\n",
    "\n",
    "### insert new features into data_dict\n",
    "count=0\n",
    "for i in data_dict:\n",
    "    data_dict[i][\"fraction_from_poi_email\"]=fraction_from_poi_email[count]\n",
    "    data_dict[i][\"fraction_to_poi_email\"]=fraction_to_poi_email[count]\n",
    "    count +=1\n",
    "\n",
    "\n",
    "### store to my_dataset for easy export below\n",
    "my_dataset = data_dict\n",
    "# print (my_dataset.head())\n",
    "\n",
    "### these two lines extract the features specified in features_list\n",
    "### and extract them from data_dict, returning a numpy array\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "\n",
    "### plot new features\n",
    "for point in data:\n",
    "    from_poi = point[1]\n",
    "    to_poi = point[2]\n",
    "    plt.scatter( from_poi, to_poi )\n",
    "    if point[0] == 1:\n",
    "        plt.scatter(from_poi, to_poi, color=\"r\", marker=\"*\")\n",
    "plt.xlabel(\"fraction of emails this person gets from poi\")\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "### if you are creating new features, could also do that here\n",
    "\n",
    "\n",
    "### split into labels and features (this line assumes that the first\n",
    "### feature in the array is the label, which is why \"poi\" must always\n",
    "### be first in features_list\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### machine learning goes here!\n",
    "### please name your classifier clf for easy export below\n",
    "\n",
    "### deploying feature selection\n",
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "### use KFold for split and validate algorithm\n",
    "from sklearn.cross_validation import KFold\n",
    "kf=KFold(len(labels),3)\n",
    "for train_indices, test_indices in kf:\n",
    "    #make training and testing sets\n",
    "    features_train= [features[ii] for ii in train_indices]\n",
    "    features_test= [features[ii] for ii in test_indices]\n",
    "    labels_train=[labels[ii] for ii in train_indices]\n",
    "    labels_test=[labels[ii] for ii in test_indices]\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(features_train,labels_train)\n",
    "score = clf.score(features_test,labels_test)\n",
    "print ('accuracy before tuning ', score)\n",
    "\n",
    "print( \"Decision tree algorithm time:\", round(time()-t0, 3), \"s\")\n",
    "\n",
    "\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "import numpy as np\n",
    "indices = np.argsort(importances)[::-1]\n",
    "#print 'Feature Ranking: '\n",
    "#for i in range(16):\n",
    "#    print \"{} feature {} ({})\".format(i+1,features_list[i+1],importances[indices[i]])\n",
    "\n",
    "# Finally I picked 10 features which are:\n",
    "# [\"salary\", \"bonus\", \"fraction_from_poi_email\", \"fraction_to_poi_email\", 'deferral_payments',\n",
    "#  'total_payments', 'loan_advances', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value']\n",
    "# Accuracy for this feature set is around 0.8.\n",
    "\n",
    "# But with these features my precision and recall were too low (less than 0.3) so I had to change\n",
    "# my strategy and manually pick features which gave me precision and recall values over 0.3.\n",
    "# In this dataset I cannot use accuracy for evaluating my algorithm because there a few POI's\n",
    "# in dataset and the best evaluator are precision and recall. There were only 18 examples of POIs\n",
    "# in the dataset. There were 35 people who were POIs in \"real life\", but\n",
    "# for various reasons, half of those are not present in this dataset.\n",
    "\n",
    "### try Naive Bayes for prediction\n",
    "#t0 = time()\n",
    "\n",
    "#clf = GaussianNB()\n",
    "#clf.fit(features_train, labels_train)\n",
    "#pred = clf.predict(features_test)\n",
    "#accuracy = accuracy_score(pred,labels_test)\n",
    "#print accuracy\n",
    "\n",
    "#print \"NB algorithm time:\", round(time()-t0, 3), \"s\"\n",
    "\n",
    "print(\"checking new features \")\n",
    "print(\"with new features\")\n",
    "fdata=data\n",
    "fdata = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier(min_samples_split=5)\n",
    "clf = clf.fit(features_train,labels_train)\n",
    "pred= clf.predict(features_test)\n",
    "acc=accuracy_score(labels_test, pred)\n",
    "print (\"Validating algorithm:\")\n",
    "print (\"accuracy after tuning = \", acc)\n",
    "print( 'precision = ', precision_score(labels_test,pred))\n",
    "print( 'recall = ', recall_score(labels_test,pred))\n",
    "print(\"without new features\")\n",
    "\n",
    "features_to_remove= [\"from_poi_to_this_person\",\"from_this_person_to_poi\"]\n",
    "without_new=  [feature for feature in features_list if feature not in features_to_remove]\n",
    "\n",
    "fdata = featureFormat(my_dataset, without_new)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier(min_samples_split=5)\n",
    "clf = clf.fit(features_train,labels_train)\n",
    "pred= clf.predict(features_test)\n",
    "acc=accuracy_score(labels_test, pred)\n",
    "print (\"Validating algorithm:\")\n",
    "print (\"accuracy after tuning = \", acc)\n",
    "print( 'precision = ', precision_score(labels_test,pred))\n",
    "print( 'recall = ', recall_score(labels_test,pred))\n",
    "\n",
    "### Tuning\n",
    "# Tuning an algorithm or machine learning technique, can be simply thought of as\n",
    "# process which one goes through in which they optimize the parameters that impact\n",
    "# the model in order to enable the algorithm to perform the best\n",
    "\n",
    "### use manual tuning parameter min_samples_split\n",
    "t0 = time()\n",
    "clf = DecisionTreeClassifier(min_samples_split=5)\n",
    "clf = clf.fit(features_train,labels_train)\n",
    "pred= clf.predict(features_test)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "acc=accuracy_score(labels_test, pred)\n",
    "\n",
    "print (\"Validating algorithm:\")\n",
    "print (\"accuracy after tuning = \", acc)\n",
    "\n",
    "# function for calculation ratio of true positives\n",
    "# out of all positives (true + false)\n",
    "print( 'precision = ', precision_score(labels_test,pred))\n",
    "\n",
    "# function for calculation ratio of true positives\n",
    "# out of true positives and false negatives\n",
    "print( 'recall = ', recall_score(labels_test,pred))\n",
    "\n",
    "\n",
    "\n",
    "# Firstly I tried Naive Bayes accuracy was lower than with Decision Tree Algorithm\n",
    "# (0.83 and 0.9 respectively). I made a conclusion that that the feature set I used does\n",
    "# not suit the distributional and interactive assumptions of Naive Bayes well enough. I\n",
    "# selected Decision Tree Algorithm for the POI identifier. It gave me accuracy before tuning\n",
    "# parameters = 0.9. No feature scaling was deployed, as it's not necessary when using a decision\n",
    "# tree. After selecting features and algorithm I manually tuned parameter min_samples_split.\n",
    "\n",
    "# min_samples_split    precision    recall\n",
    "#      2                0.67        0.8\n",
    "#      3                0.57        0.8\n",
    "#      4                0.57        0.8\n",
    "#      5                0.8         0.8\n",
    "#      6                0.8         0.8\n",
    "#      7                0.67        0.8\n",
    "#   average             0.68        0.8\n",
    "\n",
    "# It turned out that the best values for min_samples_split are 5 and 6.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### dump your classifier, dataset and features_list so\n",
    "### anyone can run/check your results\n",
    "pickle.dump(clf, open(\"my_classifier.pkl\", \"w\") )\n",
    "pickle.dump(data_dict, open(\"my_dataset.pkl\", \"w\") )\n",
    "pickle.dump(features_list, open(\"my_feature_list.pkl\", \"w\") )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
